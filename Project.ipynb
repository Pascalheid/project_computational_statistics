{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from auxiliary.auxiliary import simulate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking/ Jackknife Model Averaging and its Use in Regression Discontinuity Design\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common occurrence in data-driven Economics is that the researcher faces uncertainty about several facets of a statistical model as well as the results from its calibration. While the uncertainty in the parameter estimates of a model is typically reported by showing at least the standard deviation of the parameter estimates, it is not common practice in Economics to discuss the implications of uncertainty about the specific form of the statistical model. It is rather common to select one model among several competing models (maybe using some model selection techniques) from that on pretending that it is the true one while performing some robustness checks using other model specifications (Moral-Benito (2015)). Fletcher (2019) argues that the typical problem with this negligence of remaining model uncertainty is that parameter estimates might be biased and uncertainty is underestimated. This approach yields confidence intervals of the estimated parameters that are too optimistic in the sense that the true coverage probability is below its nominal level when not accounting for this uncertainty. This might cause the resulting inference to be seriously flawed. This aspect is a major reason why model averaging might be useful in Economics. Model averaging takes information from all competing models into account and therefore explicitly includes model uncertainty into its parameter estimates as well as its confidence intervals. Steel (2019) summarizes the tradeoff between model selection and averaging by the question what the researcher is interested in and what the previous certainty of knowledge is. If one is interested in the true model and is certain that one of the competing models must be the true one, then model selection yields the best results as it can select the true one without confounding it with the others. Although even that advantage is questionable as Zhang (2015) proves that several model avergaing estimators (among them Jackknife Model Averaging) are actually root-n consistent if the true model is among the ones that is averaged over. Further when there is considerable uncertainty about the model form, then an ensemble of those constructed by averaging yields more reliable results as it takes information from all of them into account.\n",
    "\n",
    "In Economics, the application of model averaging is imaginable in many situations. For example, in a more structural framework, often there are competing theories that inform the statistical model such as human capital as opposed to signaling theory. Another situation commonly encountered is the question which covariates should be included in a linear regression model (compare Steel (2019)). The last situation is often encountered in Regression Discontinuity Design (RDD). There is usually uncertainty about the polynomial with which the assignment variable affects the outcome variable. In the highly cited prationers' guide to RDD by Lee and Lemieux (2010) they argue that typically there is no theory available on which to base the decision for the choice of polynomial. This leads them to the suggestion to use model selection based on the Akaike information criterion (AIC) as a standard way to choose the polynomial. As there is no strong prior which polynomial to add and whether any of them is actually a good representation of the true model, for the above reasons, model averaging should be a suitable alternative to the suggested model selection procedure. It might be even superior in many situations as the actual coverage probability might be closer to its nominal one. In my first simualtion, I therefore take a typical applied RDD paper (Bronzini and Iachini (2014)) that follows quite closely the suggestions in Lee and Lemieux (2010) to benchmark whether model averaging might actually perform better than selection based on the AIC (in the sense of bias, and root mean squared error in the treatment effect as well as the coverage probability of the 95% confidence intervals). Unfortunately, I noticed only after already having set up my first simulation that there is already a published paper that has a similar idea in mind (Button (2016)). In the light of that, I adapted my first simulation study to incorporate some of the author's good ideas. For the model averaging I focus on the approach of stacking (Wolpert (1992)) which has been rediscovered in the Econometrics literature as Jackknife Model Averaging (JMA) based on Hansen and Racine (2012). My setup is also an improvement over Button's in the sense that I allow for heteroskedasticity which generally works in favor of model averaging using JMA as opposed to model selection using the AIC (I will discuss this further later). In general, my simulation differs in a few dimensions which I will elaborate on later. \n",
    "\n",
    "In a second simulation study, I go beyond the general ideas also found in Button (2016). As JMA is based on a leave one out cross validation procedure that aims at minimizing a cross validation criterion approximating the expected test mean squared error depending on the weights given to the competing models for averaging, I explore whether JMA can also be used for bandwidth selection in RDD in a self-sufficient way. As in RDD one is mainly interested in the comparison of individuals around a certain cutoff, it remains to determine which individuals to include around the cutoff to determine the treatment effect. Lee and Lemieux (2010) suggest the procedure in Imbens and Lemieux (2008) involving the use of local linear regressions around the cutoff for several bandwidths, calculating a cross validation criterion for each and then choosing the bandwidth that minimizes it. The approach in JMA is similar while accounting for different possible specifications such as polynomial ones. Hence, JMA might be an alternative for bandwidth selection (especially when local linearity is a poor approximation) which I explore in the second simulation of my project. \n",
    "\n",
    "The general structure of my project paper is the following. In the next section I introduce stacking in general and the specific implementation of it called Jackknife Model Averaging based on Hansen and Racine (2012) which involves a convenient calculation for linear regression models as needed in RDD. In section three, I introduce the general idea of Regression Discontinuity Design and explain the ideas behind Bronzini and Iachini (2014) on which my first data generating process is based. In section four I run my first simulation study in which I check the usefulness of JMA for determination of the treatment in the verge of uncertainty about the polynomial order of the assignment variable. In section five, I pick up the first simulation to see how JMA behaves as a tool for bandwidth selection and then introduce a slightly different data generating process to further pronounce its potential strength. In the last section I shortly conclude with a summary of my findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking/ Jackknife Model Averaging\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already emphasized in the introduction, model averaging has its appeal in situations where the exact form of the model is unknown. It has an advantage over model selection in the sense that it incorporates the resulting uncertainty explicitly in the modeling by combining competing models and extracting information of each of them. This makes it possible to account for the model uncertainty in the uncertainty around the estimated model parameters making inference more stable in comparison to model selection (Fletcher (2019)). Generally, there are two main strands of model averaging present. The first follows the Bayesian paradigm and is consequently called Bayesian Model Averaging. It is characterized by the fact that priors on the true parameters and the models must be formed. The outcome is a weighted average of the posteriors from different models with the weight being based on the posterior probability that a model is true. The main objective of it is to rather identify the true model as opposed to improving prediction quality. While this is generally a desirable feature for Economists, the complexity to determine the priors and the dependence of the posterior model probabilities on it makes its use difficult in practice, though (Fletcher (2019)). \n",
    "\n",
    "The second approach is Frequentist Model Averaging which is mainly concerned with improving model predictions and to obtain confidence intervals with good coverage. In this category falls the approach chosen in my project, Jackknife Model Averaging. It uses the idea of leave one out cross validation in order to obtain the weights that are used to build an optimal weighted average across different models. This idea was first introduced in Statistics as model-mixing formulated in Stone (1974). It was later picked up again by Wolpert (1992) in the realm of Machine Learning and he coined the term stacking for it. In the Econometrics literature it was Hansen and Racine (2012) who reintroduced it giving it a formal underpinning in the sense that they proved its capacity to improve model prediction. Wolpert (1992) only provides simulation evidence.\n",
    "\n",
    "In the following I will introduce the idea of stacking and especially Jackknife Model Averaging following Hansen and Racine (2012). \n",
    "Assume that we are interested in averaging across different generalized linear models, i.e. linear models that do not necessarily have errors that are normally distributed. These competing models are denoted by $m = 1, ..., M$. The true data generating process takes the general linear form as following:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        y_i = \\mu_i + e_i \\\\\n",
    "        E(e_i|x_i) = 0.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "The data is independently distributed $(x_i, y_i)$ for $n = 1, ..., N$. $\\mu_i$ corresponds to $E(y_i|x_i)$. We determine $y = (y_1, ..., y_N)'$, $\\mu = (\\mu_1, ..., \\mu_N)'$ and $e = (e_1, ..., e_n)'$. Furthermore there is, potentially, heteroskedasticity defined as $E(e_i^2|x_i)=\\sigma_i^2$. Each of the competing models now differ in the sense that they yield possibly different linear estimators ${\\hat\\mu^1, ..., \\hat\\mu^M}$ for $\\mu$.\n",
    "Moral-Benito (2015) note that the general approach of Frequentist Model Averaging is to now find a weighted average among those estimators $\\hat\\mu^m$ sucht that the prediction of the ensemble is optimized. This means the general idea is to find weights $w^m$ per model that yield an ensemble estimator like the following:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat\\mu(w) = \\sum_{m=1}^{M} w^m \\hat\\mu^m = \\hat\\mu w.\n",
    "\\end{equation}\n",
    "\n",
    "The idea of Frequentist Model Averaging, hence, boils down to finding a weigthing vector $w=(w^1, ..., w^M)'$ based on optimizing some criterion that depends on the weights. In the case of stacking/ JMA this criterion is based on leave one out cross validation (as Fletcher (2019) Hansen and Racine (2012) wrongly use the term jackknife for it, I refrain from following their terminology in the following). Before I derive the exact cross validation procedure as well as the resulting cross validated criterion, let us introduce some further notation. Hansen and Racine focus their attention on the linear regression model for the application of the JMA and introduce an efficient calculation of their criterion for this special class of models. In general, in the class of linear models, the estimator $\\hat\\mu^m$ can also be written as a transformation of the vector $y$ using a projection matrix $P_m$ , i.e. $\\hat\\mu^m = P_m y$. It is well-known that for linear regression the matrix $P_m$ has the form $P_m = X^m(X^{m'}  X^m)^{-1} X^{m'} $. The ensemble estimator can in general be written as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat\\mu(w) = P(w) y \n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{equation}\n",
    "    P(w) = \\sum_{m=1}^{M} w^m P_m.\n",
    "\\end{equation}\n",
    "\n",
    "The first observation here is that $P(w)$ is linear in $w$ and $y$ is linear in $P(w)$ which means that it is also linear in $w$. Hansen and Racine (2012) restrict the weights to lie in a unit simplex, i.e. to lie in:\n",
    "\n",
    "\\begin{equation}\n",
    "    H_n = \\{w \\in R^M: w^m \\geq 0, \\sum_{m=1}^{M} w^m = 1\\}.\n",
    "\\end{equation}\n",
    "\n",
    "The leave one out cross validation now comes into play in how the estimator for $\\mu$ is calculated for each model. The linear estimator of the m*th* model is the following $\\tilde \\mu^m = (\\tilde \\mu_1^m, ..., \\tilde \\mu_N^m)'$. $\\tilde \\mu_i^m$ is the fitted value of model $m$ for observation $i$ when estimating the model on the data set without observation $i$ and then applying the estimated model on the i*th* observation. From this vector of linear estimates one can derive the cross validated residual vector $\\tilde e^m = y - \\tilde \\mu^m$. As we have $M$ models in total, mixing them with some weight vector $w$ results in different cross validated linear estimators $\\tilde \\mu(w)$ and consequently different cross validated residuals $\\tilde e(w)$ depending on the weight. This is depicted in the following two equations:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\tilde \\mu(w) = \\sum_{m=1}^M w^m \\tilde \\mu^m = \\tilde\\mu w = \\tilde P(w) y \\\\\n",
    "        \\tilde e(w) = y - \\tilde \\mu(w) = \\sum_{m=1}^M w^m\\tilde e^m = \\tilde e w\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "with $\\tilde \\mu = (\\tilde \\mu^1, ..., \\tilde \\mu^M)'$, $\\tilde P(w) = \\sum_{m=1}^M w^m \\tilde P_m$ and $\\tilde e = (\\tilde e^1, ..., \\tilde e^M)'$. \n",
    "Taking a step back to general stacking and how JMA relates to it, let us look at how stacking is typically built up. Stacking seeks to maximize the following expression by setting the weights $w^m$: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\sum_{n=1}^N log L(\\tilde \\mu_i(w)|y_i)\n",
    "\\end{equation}\n",
    "\n",
    "where $log L(.)$ describes the log likelihood function and $\\tilde \\mu_i(w) = \\sum_{m=1}^M w^m \\tilde \\mu_i^m$ which is the averaged linear estimator for observation $i$ calculated using the data set without the i*th* observation. The approach taken by JMA is to now rely on another criterion than the log likelihood $log L(.)$ above but rather the least squares cross validation criterion as an estimate for the expected true error:\n",
    "\n",
    "\\begin{equation}\n",
    "    CV(w) = \\frac{1}{n} \\tilde e(w)' \\tilde e(w) = w' S w\n",
    "\\end{equation}\n",
    "\n",
    "where $S = \\frac{1}{n} \\tilde e' \\tilde e$. This means that Hansen and Racine (2012) construct a measure for the expected test mean squared error depending on the extent to which the individual models are mixed. $CV(w)$ is an $M \\times M$ matrix covering the extreme cases that might occur on the diagonal which capture choosing weight of one for either of the models. As the choice of $w$ is restricted to lie in $H_n$ and the expected test error is supposed to be minimized this comes down to a constraint optimization problem like the following:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\textrm{min}_w CV(w) \\\\\n",
    "        \\textrm{subject to } w \\in H_n.\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "This is a quadratic programming problem yielding an optimal weight $\\hat w$ which can then be used to derive the JMA estimator for $\\mu$ which is $\\hat \\mu(\\hat w) = \\hat \\mu \\hat w$.\n",
    "\n",
    "In order to summarize, the approach involves setting up each potential model indivdually. For each model based on leave one out cross validation the fitted value of each observation $i$ has to be obtained by estimating the model without that very observation. This means that per model $N$ regressions have to be run. Based on these fitted values, the cross validated residuals are derived which are then together with the potential weights $w$ are used to build the least squares cross validation criterion. This serves an approximation of the expected test mean squared error of the resulting ensemble depending on the choice of $w$. Consequently, it is minimized by choosing optimal wights $\\hat w$ under the constraint that the each of them are nonnegative and sum up to one together.\n",
    "For the case of linear regression, Hansen and Racine (2012) come up with a computationally light way of performing leave one out cross validation that reduces it to one operation per model as opposed to $N$ regressions. We already observed that in linear regressions the projection matrix has the form $P_m = X^m(X^{m'}  X^m)^{-1} X^{m'}$. This means consequently that the cross validated estimator for observation $i$ of a model $m$ is the following: $\\tilde \\mu_i^m = x_i^{m'} (X_{(-i)}^{m'} X_{(-i)}^{m})^{-1} X_{(-i)}^{m'} y_{(-i)}$ where $(-i)$ means that it is the data set without observation $i$ and $x_i^m$ is the i*th* row of the regressor matrix. The authors now make use of a derivation in Li (1987). Li derives that the cross validated projection matrix $\\tilde P_m$ has the following form in linear regressions $\\tilde P_m = D_m (P_m - I) + I$ with \n",
    "\n",
    "\\begin{equation}\n",
    "    D_m = \\begin{pmatrix} (1-h^m_{11})^{-1} & 0 & \\cdots & 0 \\\\ 0 & \\ddots & & \\vdots \\\\ \\vdots & & \\ddots & 0 \\\\ 0 & \\cdots & 0 & (1-h^m_{NN})^{-1} \\end{pmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "and $h^m_{ii} = x_i^{m'} (X^{m'} X^{m})^{-1} x_i^{m}$ which is also the i*th* diagonal element of $P_m$. The cross validated residual vector of a single model $m$ can hence be written as $\\tilde e^m = D_m(y - P_m y)$ which can be obtained in a single operation per model. This is exploited in the code I have written and exhibits a large speed advantage over the conventional way of performing leave one out cross validation which I will also explore in one of my simulations. \n",
    "\n",
    "While the Jackknife Model Averaging is a form of stacking, Hansen and Racine (2012) now add an entirely new element to the discussion which is that the JMA weights $\\hat w$ are asymptotically optimal drawing on two criteria. They define the training mean squared prediction error as $L_n(w) = \\frac{1}{n} (\\mu - \\hat \\mu(w))'(\\mu - \\hat \\mu(w))$ and the expected test mean squared prediction error as $R_n(w) = E(L_n(w)|X)$. Under some regularity conditions, they prove that these criteria converge in probability to their minimal possible value across all feaasible $w \\in H_n$ when emplyoing the JMA weights $\\hat w$. Formally, this looks like the following: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{L_n(\\hat w)}{\\text{inf}_{w \\in H_n} L_n(w)} \\rightarrow_p 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{R_n(\\hat w)}{\\text{inf}_{w \\in H_n} R_n(w)} \\rightarrow_p 1.\n",
    "\\end{equation}\n",
    "\n",
    "This shows that asymptotically the JMA weights yield the best in-samle and out-of-sample prediction across all feasible weights which also includes estimating just a single one of each model. This showd clearly that the focus of the approach in now improving predictive performance. This is achieved by a decrease in variance at the cost of increasing bias. \n",
    "While we are generally more interested in measuring an accurate treatment effect in RDD, I will argue in the next section why JMA might still be beneficial for RDD although might induce some bias in the treatment effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Discontinuity Design\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data generating process is based on a quite typical setup in RDD which is inspired by Bronzini and Iachini (2014). In their paper they pursue the question whether subsidy programs for Research & Development (R&D) for companies is effective in the sense that they actually induce firms to subsequently raise their investment in R&D. Essentially, they are interested in measuring the treatment effect of subsidies for R&D on the actual investment in it. FOr that they exploit a specific subsidy program that was launched in Northern Italy. The program began in 2003 and ask industrial companies to come up with project ideas which that were funded by the program in case the idea gained a certain score determined by an independent commission. Every firm that received 75 points or more for their idea was subsidized by a percentage of the total amount of the project. \n",
    "The authors' identification strategy now involves a sharp RDD which postulates that those firms around the threshold of 75 points of score are comparable. It is argued that this has quasi-experimental character allowing the authors to identify the treatment effect by comparing some measure of R&D investment across firms around the threshold. \n",
    "The authors draw from several measures while focussing on investment into R&D relative to the sales before the subsidy program was launched in order to make it comparable. This is also the dependent variable I will focus on. In general in RDD, this dependent variable is a function of the assignment variable (that determines whether the firm is assigned to treatment) which is the score here. If the firms cannot control the assignment process, the RDD approach is valid. By assuming that the assignment around the threshold is somewhat random rendering the firms around it very similar apart form differeng in treatment and control, one can compare those at the threshold by measuring the discontinuity in the function of the dependent variable on the assignment variable. In this specific application, the authors allow for treatment effect heterogeneity. They allow the treatment effect to vary depending on whether the firm is classified as small or large. It is also quite common to further control for some variables that might affect both the dependent and the assignment variable which is not done in this paper. As previously noted, it is not ad-hoc clear whether the dependent variable is linear in the assignment variable or not. For this reason, it is common practice (promoted by Lee and Lemieux (2010)) to add several polynomials of the assignment varible to the function and deciding on the \"true\" model by choosing the one that has the lowest negative AIC. Formally, the described regression in the paper at hand looks like the following:\n",
    "\n",
    "\\begin{equation}\n",
    "    Y_i = \\sum_{k=1}^2 \\sum_{p=0}^P \\alpha_{kp} Size_i^k (X_i-c)^p + T_i \\sum_{k=1}^2 \\sum_{p=0}^P \\beta_{kp} Size_i^k (X_i-c)^p +   \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "where $Size_i^1 = 1$ if the firm $i$ is small and zero otherwise and $Size_i^2 = 1$ if firm $i$ is large and zero otherwise. $Y_i$ is the investment in R&D divide by pre-program sales and $X_i$ is the assignment varible which is in our case the score a firm received for its project idea. $h$ captures the threshold for the assignment of the treatment which is equal to 75 as every firm having at least this score obtained the subsidy. $T_i$ is the treatment indicator which is one if $X_i \\geq 75$ and zero otherwise. Withdrawing the threshold $c$ from the assignment variable $X_i$ is common practice as like this the coefficients $\\beta_{10}$ and $\\beta_{20}$ capture the discontinuity (the difference in intercept) at the threshold between treatment and non treatment for small and large firms, respectively. Consequently, they capture the treatment effect. The $P$ displays that the order of polynomial included in the regression function is unknown and potentially different ones might be included.\n",
    "\n",
    "The authors now find themselves in a common situation when employing RDD. They cannot rely on any theory of how the assignment variable affects the dependent variable. Capturing the functional form as precisely as possible is of large interest, though, as it affects how accurately the difference in intercept and hence in treatment effect can be measured. For this reason, usually, it is allowed to have potentially different functional forms on each side of the cutoff. The flexibility in function form is typically tested by including more and more polynomials of the assignment variable one at a time. The argument for this approach is that, when running only a polynomial of order one while the true relationship actually is more nonlinear might result in strong ommited variable bias. This is where model averaging and specifically JMA can come into play. Although model averaging increases bias while reducing variance in the estimates, by including information from several models (several polynomials) in decrease the danger of omitted variable bias. Further if the true model is among the candidates, the JMA is actually root-n consistent which means that with large n it does not do any harm to use JMA as opposed to luckily only running the true model. In comparison to model selection based on AIC as suggested in Lee and Lemieux (2010) it has the advantage that information from all models enters the parameter estimates and its uncertatinty. In the paper at hand, running the regression above with up to three polynomials and select the correct model via AIC suggests that the true model has polynomial of zero for small firms and the treatment effect is measured at 0.045 with a standard error of 0.018. This is just significant at the five percent level and rests heavily on the accurracy of both point estimates. As already established before, model selection tends to display too much certainty, understating standard errors and confidence interval. In the example above it might easily be the case that statistical significance cannot be maintained when incorporating model uncertainty. This is where JMA with its more accurate confidence intervals could be beneficial, although the bias might be increased but still it could give more accurate inference than AIC model selection and display less risk of ommited variable bias than single estimation of one model. In my first simulation study I will investigate the difference of treatment effect between AIC model selection and JMA based on setup of Bronzini and Iachini (2014). \n",
    "\n",
    "Lee and Lemieux (2010) discuss a second major method of how to estimate the treatment effect in RDD which gives rise to my second simulation study. They argue that the above regression is not too appelaing in the sense that it uses data far away from the cutoff point to predict the dependent variable at the cutoff (which one is solely interested in). It is therefore argued that an approach which reduces the data to that closer to the cutoff might be prefered. Usually relied on when pursuing this is the nonparametric method suggested by Imbens and Lemieux (2008). The crucial part of this approach is the choice of the bandwidth $h$ around the cutoff $c$ for which the data is included in the nonparametric regression. The idea is to restrict the data to close around the cutoff, i.e. including all $i$ for which $c-h \\leq X_i \\leq c+h$. Within this window, the data is used to run a kernel regression with a rectangular kernel which essentially boils down to running a regular linear regression as above with $P=1$ with the only difference that they run seperate regressions for each side of the cutoff. These regressions are run across a range of different bandwidth and for each of the test mean squared error is calculated via leave one out cross validation. Averaging those across both sides of the cutoff results in a single criterion per bandwidth which looks like the following: \n",
    "\n",
    "\\begin{equation}\n",
    "    CV_Y(h) = \\frac{1}{N} \\sum_{i=1}^N (Y_i - \\hat Y(X_i))^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat Y$ is calculated as in JMA with leave one out cross validation based on a local linear regression on each side of the cutoff $c$ with a certain bandwidth $h$. The optimal bandwidth is then the $h$ that minimizes the above criterion, i.e. $h^* = \\text{argmin}_h CV_Y(h)$. Clearly, there is a resemblence between this criterion and the one used in JMA. For this reason, I try whether JMA might be used to run local polynomial regressions in order to select the optimal bandwidth. This might advantegeous in the case where the linear approximation the local linear regression relies on is actually in parts not accurate. I would generally expect that JMA might actually perform better than the above approach when there polynomial nonlinearity to varying degree across the range of different bandwidths tested. This idea is investigated in my second simulation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Study One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first simulation study I mimic the true data of the Bronzini and Iachini (2014) for the independent variables as closely as possible. The regressors in the paper are only the size of the firm and the score. As there are roughly 50 percent small firms I draw the size from a binomial distribution with probability of 0.5. For $N$ observations this corresponds to:\n",
    "\n",
    "\\begin{equation}\n",
    "    Size^1 \\text{~} B(N, 0.5).\n",
    "\\end{equation}\n",
    "\n",
    "There are some differences regarding the average distribution of the score depending on the size. So I simulate the score depending on the size of the firm. I use a right skewed normal distribution with $N(87, 15)$ for 80 percent of the total small firms in combination with a uniform distribution $U(20, 55)$ for the rest. For large firms I only take a right skewed normal distribution of $N(92, 20)$. As the scores can only take discrete values between 0 and 100, I round every number to the nearest integer and make sure that all observations lie in the interval by replacing values outside by a random choice of those values already drawn inside the interval. The comparison of the original data with my simulated data is shown in the graphs below. For the the regressors the distributions are held stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original data\n",
    "original_data = pd.read_stata(\"data/Bronzini-Iachini_dataset.dta\")\n",
    "# simulate data\n",
    "num_obs = 360\n",
    "coefficients = {\"untreated\": np.array([-0.05, -0.02]),\n",
    "                \"treated\": np.array([0.08, 0.03])}\n",
    "simulated_data = simulate_data(num_obs, coefficients)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeVxU5f7A8c8XGDZBQMEVFdw3UAHFLcVSc9+yxay0btl6q3vztlp2K9uuv255y7pl6a00K03TNM01zVwCd8F9A0VFkH1nnt8fZzRE0AGRmYHn/XrNa5g5zznne1C+88xznkWUUmiapmnVl5OtA9A0TdNuLJ3oNU3Tqjmd6DVN06o5neg1TdOqOZ3oNU3Tqjmd6DVN06o5neg1rRgRCRIRJSIultfrReTBch7jExF5+cZEqGnlpxO95hBEpLeI/C4iaSKSIiKbRKSrjWN6VUQKRCSz2ONZpdQjSqnXbRmbphXnYusANO1aRKQ28BPwKPAd4ArcBOTZMi6Lb5VS91hbWERclFKFNzIgTStJ1+g1R9AaQCn1jVKqSCmVo5T6RSm1G0BEJlpq+P8WkVQROSoiPS3vx4vIORGZcPFgIjJURHaISLpl+6uVGayIzBGRNyw/R4lIgog8JyJngNmWbwLfi8jXIpIhIntEpLWIvGCJNV5EBhY73kTLNWWIyDERGV+Z8WrVn070miM4CBSJyP9EZLCI+JVSJhLYDdQF5gHzga5AS+Ae4EMR8bKUzQLuA3yBocCjIjLqBsbfAKgDNAMmWd4bDnwF+AE7gJUYf4+NgdeA/wKISC1gBjBYKeUN9AR23sBYtWpIJ3rN7iml0oHegAI+A5JEZImI1C9W7JhSarZSqgj4FmgCvKaUylNK/QLkYyR9lFLrlVJ7lFJmy7eCb4C+FQzvDsu3iIuPRqWUMQNTLbHkWN7bqJRaaWnG+R4IAN5WShVgfEgFiYhvsf07ioiHUipRKbWvgrFqNZRO9JpDUErFKaUmKqUCgY5AI+D9YkXOFvs5x7JPyfe8AEQkUkTWiUiSiKQBjwD+FQztO6WUb7HH6VLKJCmlcku8VzK285YPqUvxA15KqSzgTkuMiSKyTETaVjBWrYbSiV5zOEqp/cAcjIRfEfOAJUATpZQP8AkglRNdqa5rilhLzX8A0BDYj/GtRtOsphO9ZvdEpK2IPCMigZbXTYBxwJYKHtIbSFFK5YpIN+DuSgq10olIfREZYWmrzwMygaJr7KZpl9GJXnMEGRg3W7eKSBZGgt8LPFPB4z0GvCYiGcArGF027ZUTxnWeBlIw7iU8ZtOINIcjeuERTdO06k3X6DVN06o5neg1TdOqOZ3oNU3Tqjmd6DVN06o5u5zUzN/fXwUFBdk6DE3TNIcRExNzXikVUNo2u0z0QUFBREdH2zoMTdM0hyEiJ8rapptuNE3Tqjmd6DVN06o5u2y60TTN/uQWFLH+QBL7TqdxJCmTIrPC1cWZVvW86NTElx7N6+LqouuO9shhEn1BQQEJCQnk5pacBFCrCHd3dwIDAzGZTLYORbNzZ9Nz+XDtYRbvPEVGbiHOTkKzOp6YnJ3ILihk6S5jws46tVwZ06UxD/dtQYC3m42j1opzmESfkJCAt7c3QUFBiNzIiQarP6UUycnJJCQkEBwcbOtwNDtVZFbM2niUGWsOUVCkGBrakLHhgYQ388Pd5HypXEZuAX8cT+H76ATm/H6c+X/E89ebW/KX3sG4OOsavj1wmESfm5urk3wlERHq1q1LUlKSrUPR7NSFrHyenL+DjYfO079dfaYMbUeQf61Sy3q7m7i5bX1ublufo0mZTFsWx1s/72f9gSRmjOuia/d2wKE+bnWSrzz6d6mV5URyFsM//I2tR1N4a0wIsyZElJnkS2oe4MXnE7vyr7GhbD95gWH/2cihsxk3OGLtWhwq0WuadmMdScrkzv9uISuvkG8f7s64bk0rdJzbI5qw6LFemBWM+2wLB87oZG9LOtFXsiFDhpCamnrVMq+88gqrV6+u0PHXr1/PsGHDKrSvpl1NfEo24z7dQqHZzDeTutOlaWlrsFuvfaPazJ/UHScRxn22hZPJ2ZUUqVZeOtFXEqUUZrOZ5cuX4+vre9Wyr732Gv3796+iyDTt2tKyC7h/zh/kFhQx76HutG1Qu1KO2yLAi/mTulNkVjz0ZTSZeYWVclytfHSiL4f33nuPjh070rFjR95//32OHz9Ou3bteOyxxwgLCyM+Pp6goCDOnz8PwOuvv07btm0ZMGAA48aNY/r06QBMnDiRBQsWAMZ0D1OnTiUsLIyQkBD2798PwLZt2+jZsyddunShZ8+eHDhwwDYXrVV7BUVmHv46mpPJ2Xx6XwSt63tX6vGbB3gxc3wYh5MyeXr+TvRiR1XPYXrdFPfPpfuIPZ1eqcds36g2U4d3KHN7TEwMs2fPZuvWrSiliIyMpG/fvhw4cIDZs2czc+bMy8pHR0ezcOFCduzYQWFhIWFhYYSHh5d6bH9/f7Zv387MmTOZPn06s2bNom3btmzYsAEXFxdWr17Niy++yMKFCyv1mjUN4J2f97PlaArv3dGJ7s3r3pBz9Grpz0tD2vHaT7F8veUE9/YIuiHn0UrnkIneFn777TdGjx5NrVpG74MxY8awceNGmjVrRvfu3UstP3LkSDw8PAAYPnx4mcceM2YMAOHh4fzwww8ApKWlMWHCBA4dOoSIUFBQUNmXpGks253IrN+OMaFHM8aEBd7Qc93fK4j1B5N4c/l+bmoVYHVPHu36OWSiv1rN+0Yp6+vmxcRvbfnSuLkZ/YydnZ0pLDTaMF9++WX69evHokWLOH78OFFRUeULWNOuIT4lm+cW7qZLU19eGtr+hp9PRHj3tlAG/vtXnvl+F98/3AMnJ93NtyroNnor9enTh8WLF5OdnU1WVhaLFi3ipptuKrN87969Wbp0Kbm5uWRmZrJs2bJynS8tLY3GjRsDMGfOnOsJXdOuUGRWPPPdLgBm3NWlyuaoaeDjztThHYg5cYEFMQlVck7NykQvIoNE5ICIHBaR50vZLiIyw7J9t4iEWd53F5FtIrJLRPaJyD8r+wKqSlhYGBMnTqRbt25ERkby4IMP4udXdvezrl27MmLECDp16sSYMWOIiIjAx8fH6vM9++yzvPDCC/Tq1YuioqLKuARNu2TWxqNsO57CqyM60KSOZ5Wee0xYY8Kb+fHuyv2k5+omySqhlLrqA3AGjgDNAVdgF9C+RJkhwM+AAN2BrZb3BfCy/GwCtgLdr3XO8PBwVVJsbOwV79m7jIwMpZRSWVlZKjw8XMXExNg4oss54u9Uu36xp9NUqxeXq4e/jFZms7niBzKblSoqrNCuu+NTVdDzP6nXl+6r+Pm1ywDRqoycak0bfTfgsFLqKICIzAdGArHFyowEvrScbIuI+IpIQ6VUIpBpKWOyPGpM36pJkyYRGxtLbm4uEyZMICwszNYhaTVcXmERf/t2J7U9TLw5JqR8U2GYi+DYBtjzPSTuhtQTkJcBXvXBtwk07wed7oK6La55qJBAH+6MaMKc348zoWdQlX+rqGmsSfSNgfhirxOASCvKNAYSRcQZiAFaAh8ppbaWdhIRmQRMAmjatGLDru3NvHnzbB2Cpl3mvV8Osv9MBrMndqVOLVfrdjKbYefXsP5tSD8Fbj7QNBKa9QB3X8g4DclHYeN02PAuBPeFwe9CvbZXPezT/Vvzw45T/GftId4d26kSrk4rizWJvrSP/JK18jLLKKWKgM4i4gssEpGOSqm9VxRW6lPgU4CIiIgaU+vXtKqyKz6VzzYeZVy3pvRrW8+6nU7vgKVPQ+JOaBIJt06D1oPB5H5l2fRE2D0ffnsfPukFPf8KUS+CS+kfKA183Bkf2ZQvN5/gsaiWurvlDWTNzdgEoEmx14HA6fKWUUqlAuuBQeWOUtO061JQZOa5hbsJ8HbjhSFXr2lfEjMHPh8ImWdhzCx4YCV0GF16kgeo3RB6/w3+GgOhd8Jv/4avRkN2SpmneDSqBSZn4YM1h8p/UZrVrEn0fwCtRCRYRFyBu4AlJcosAe6z9L7pDqQppRJFJMBSk0dEPID+wP5KjF/TNCt8uuEo+89k8PrIjtR2v8aqYkWFsORJWPoUBPWGR3+H0NvB2vb8Wv4waiaM+QwStsHnAyDlaKlF63m7c1+PIH7ceYr4FD3p2Y1yzUSvlCoEngBWAnHAd0qpfSLyiIg8Yim2HDgKHAY+Ax6zvN8QWCciuzE+MFYppX6q5GvQNO0qjiZl8sGaQwwJacDADg2uXrgwHxbcD9v/Z9TOxy8AzzoVO3HoHXDfEqNGP2c4pJXeb/4vvYNxdhI+/+1Yxc6jXZNV/eiVUsuVUq2VUi2UUtMs732ilPrE8rNSSj1u2R6ilIq2vL9bKdVFKRWqlOqolHrtxl1K1XvwwQeJjY29dkErFJ8MrSxvvvlmuY87Z84cnnjiiYqGpTk4s1nxwg97cHdx4tUR1xhRXpgH390HcUvg1reg/6vg5Hz1fa6lWQ+470fISzeacbKSryhSv7Y7Izs35ts/4rmQlX9959NKpUfGXodZs2bRvv2NHzp+UUUSvVazfRsdz9ZjKbw0tB31vMtoWwej6+TCB+HgzzD0PejxWNlly6thKIybD6knYd7tUJB7RZFJfZqTU1DE11tOVN55tUt0ordSVlYWQ4cOpVOnTnTs2JFvv/2WqKgooqOjAfDy8uK5554jPDyc/v37s23bNqKiomjevDlLlhi3NErWrocNG8b69euvONeoUaMIDw+nQ4cOfPrppwA8//zz5OTk0LlzZ8aPHw/A119/Tbdu3ejcuTMPP/zwpRG0s2fPpnXr1vTt25dNmzbdyF+LZsfOpefy5vI4ejSvyx0RTcouqBSseN6oyQ96G7r+pfKDCeoFt30Op2Lg539csbl1fW/6tQngf5uPk1eoR4JXNoec1Iyfn4czeyr3mA1CYPDbZW5esWIFjRo1ujRnTVpaGh9//PGl7VlZWURFRfHOO+8wevRopkyZwqpVq4iNjWXChAmMGDHC6lC++OIL6tSpQ05ODl27duW2227j7bff5sMPP2Tnzp0AxMXF8e2337Jp0yZMJhOPPfYYc+fOZcCAAUydOpWYmBh8fHzo168fXbp0qeAvRXNkU5fsI7/QfO2BUb/PgG2fQo8noPujNy6gdsPgpmdg4/8ZXTW73HPZ5om9gpnwxTZW7D3DyM6Nb1wcNZCu0VspJCSE1atX89xzz7Fx48Yr5q1xdXVl0KBBl8r27dsXk8lESEgIx48fL9e5ZsyYQadOnejevTvx8fEcOnRl17M1a9YQExND165d6dy5M2vWrOHo0aNs3bqVqKgoAgICcHV15c4776zwNWuOa8XeM/y89wxP9W9F8NX6px/8BVZNNbpNDnj9xgfW7yUI7gPLnrmisnZTS3+C6nry5WbdfFPZHLNGf5Wa943SunVrYmJiWL58OS+88AIDBw68bLvJZLpUa3Jycro09bCTk9OlqYddXFwwm82X9snNvbKtcv369axevZrNmzfj6elJVFRUqeWUUkyYMIG33nrrsvcXL15cvmHtWrWTllPAKz/upV3D2jx0U/OyCyYfMdrlG3SEkTPBqQrqfU7OcNsXxoCqRY/AQ+suDahychLu7RHE6z/FsvdUGh0bWz8JoHZ1ukZvpdOnT+Pp6ck999zD5MmT2b59e7mPERQUxM6dOzGbzcTHx7Nt27YryqSlpeHn54enpyf79+9ny5Ytl7aZTKZLC5DccsstLFiwgHPnzgGQkpLCiRMniIyMZP369SQnJ1NQUMD3339fwSvWHNU7K/ZzPjOPd24LweRcxp94XibMH28k9zvngmsVzjXjFQDD3oeze2HDvy7bNDY8EA+TM1/pWn2l0oneSnv27Ll043PatGlMmTKl3Mfo1asXwcHBhISEMHny5FInORs0aBCFhYWEhoby8ssvX7Z61aRJkwgNDWX8+PG0b9+eN954g4EDBxIaGsqAAQNITEykYcOGvPrqq/To0YP+/fvridRqmK1Hk5m39SR/6R1MaOBVFqlf/g9I2g9jvwC/ZlUX4EVth0CncUZ7/ekdl9728TAxsnMjluw6TYaewrjSiLLDhXojIiLUxd4sF8XFxdGuXTsbRVQ96d9p9ZJbUMSQDzZSYDaz8uk+eLqW0TK7az4sehj6Pgf9XqzaIIvLSYWZ3cGjDjz8KzgbI3a3n7zAmJm/89aYEMZ1qx4THFYFEYlRSkWUtk3X6DWtmvhw7WGOns/izdEhZSf584fhp79Ds17Q59mqDbAkD18YMh3O7YOt/730dpcmvrSq58V30fFX2VkrD53oNa0aiEtM55NfjzAmrDE3tQoovVBRASz8i3Hzc8xn4GwHfTHaDoVWA2H9W5BuzIMoItwR0YQdJ1M5dDbDxgFWDw6V6O2xmclR6d9l9VFkmebAx8PEy1db5HvDdGO64eEfgI+d9FMXgcHvGB9CK1+69PbosMa4OImu1VcSh0n07u7uJCcn6wRVCZRSJCcn4+5+lSHxmsOYt/UEO+NTmTKsHX5lLSZyKsbo4RJ6F7QfWbUBXkud5sZAqn0/GCtYAf5ebtzSrh4/bD9FQZH5GgfQrsUOvrtZJzAwkISEBJKSkmwdSrXg7u5OYGCgrcPQrtO59FzeXXGAXi3rMqqs0aQFOfDDw+DdwKg926NeT8KOr2DlizDpV3By5s6uTVi57yxr4s4xqOM1Zt3UrsphEr3JZCI4ONjWYWiaXfnnT7HkFZl5Y9RVpjlY/U9IPgT3LjZugNojk4cxW+bCvxi9grqMp0+rAOp5u/F9dLxO9NfJYZpuNE273LoD51i2O5En+rUse5qDo7/C1o+h28PQol/VBlheHW+DxhGw9nXIz8LF2Ymx4YGsO3COs+lXjg7XrKcTvaY5oJz8Il5evJfmAbV4uG8Z0xzkpsHix6BuS6O2bO9E4NY3ISMRfv8QgNsjmmBWsHB76YuWaNbRiV7THNCMtYdIuJDDm6NDcHMpY3GQFS8YSXP0p1U7xcH1aBoJ7UbA7/+BrGSC/WvRLbgO30cn6I4Y18GqRC8ig0TkgIgcFpHnS9kuIjLDsn23iIRZ3m8iIutEJE5E9onIU5V9AZpW0xw4k8FnG44yNjyQ7s3rllFoBeycaywHGBhetQFer34vQUEWbPo3AGPDAjl2PovtJ1NtHJjjumaiFxFn4CNgMNAeGCciJTvrDgZaWR6TgIsTtRcCzyil2gHdgcdL2VfTNCuZzYoXF+3B292FF4eUMX1FdgosfRLqdzSmOXA09doa3UC3fQbppxkS2hAPk7NuvrkO1tTouwGHlVJHlVL5wHygZEfckcCXlrVjtwC+ItJQKZWolNoOoJTKwFhc3E5Gamia4/k2Op6YExd4cUg76pTVZ/7nZyE7GUZ9fGkKYIcT9ZyxvOGGf+Hl5sKgjg1Yuus0uQV69amKsCbRNwaKD09L4Mpkfc0yIhIEdAG2lnYSEZkkItEiEq37ymvalZIy8nhreRyRwXUYG17GGIjYJbDne2Mem4ahVRtgZfILgvCJsP1LSDnKbWGBZOQWsir2rK0jc0jWJPrSOueWvCty1TIi4gUsBJ5WSqWXdhKl1KdKqQilVERAQBlzdWhaDTZtWSw5BUVMG11Gn/ms8/DT36BhJ7jp71UfYGXrMxmcTLDuLXq0qEtDH3fdfFNB1iT6BKD4ysKBwGlry4iICSPJz1VK/VDxUDWt5vrt0HkW7zzNo1EtaVnP68oCSsGyv0NeOoz65NKUvw7NuwFEPgx7vsc5KZYxYY3ZcDCJc7pPfblZk+j/AFqJSLCIuAJ3AUtKlFkC3GfpfdMdSFNKJYpR7fgciFNKvVepkWtaDZFbUMSUxXsIquvJY1EtSi+0dyHE/ghRL0D9atTfoddT4OYNa6cxJiwQs4LFO0/ZOiqHc81Er5QqBJ4AVmLcTP1OKbVPRB4RkUcsxZYDR4HDwGfAY5b3ewH3AjeLyE7LY0hlX4SmVWcz1x/heHI2b4wKwd1USp/5jLOwfLIxqrTnk1Uf4I3kWce4pgPLaJG3ny5NfVkQo/vUl5dVc90opZZjJPPi731S7GcFPF7Kfr9Revu9pmlWOJKUySfrjzCqcyN6t/K/soBS8NPTxsRloz62jznmK1v3R2HrJ7D2DcaGf8BLi/ay91Q6IYF68XBr6ZGxmmanlFK8tGgP7iYnXiprnvld8+HAcrj5ZQhoXbUBVhU3L2Pg19F1jPQ9hquLk74pW0460WuanVq04xRbjqbw3OC2BHi7XVkg5ajRZNO0p1Hrrc66/gW8GuC16R0GtKvHjztPkV+o56m3lk70mmaHUrPzmbYsji5NfRnXtZQFsgvzYcED4OQCYz4FpzLmu6kuTB5Gd8uTv/NQ4xNcyC5g7f5zto7KYehEr2l26O2f95OaU8Cbo0NwcirlNtfa1+H0DhjxH/BtcuX26ijsPvBpQqeDHxLg5aqbb8pBJ3pNszPRx1OY/0c8f+kdTLuGta8scHgN/D4DIh6A9iOqPkBbcXGDvs8ip2P4R/Ax1u0/R3Jmnq2jcgg60WuaHSkoMvPSor008nHnqVtaXVkg8xwsegQC2hlzt9c0ncaBXzAjUmZTZC7ix50lx25qpdGJXtPsyOe/HePA2Qz+ObIjtdxKdJU0m40kn5cOY78w2q1rGmcTRL2Ae3IsDwfs47voeN2n3go60WuanYhPyeb91QcZ2L4+A9rXv7LApn/DkTVGTb46jX4tr5Cx4N+GR9W3HDyTxvaTF2wdkd3TiV7T7IBSiqlL9uEkwqsjOlxZ4NBqWPO6sa5qxANVH6A9cXKGfi/ik3mUO9y2MHfrSVtHZPd0otc0O/Dz3jOs3X+Ovw9oTSPfEk0yKUdh4QNQv4PRy6a0mStrmnYjoH4Ik90Ws2J3PKnZ+baOyK7pRK9pNpaanc8rP+6lY+PaTOwZdPnG3DSYPx4QuPNrcK1lixDtj5MT3PwS/vkJDFO/snC7nujsanSi1zQbe+2nWFKzC3j3tk64OBf7kywqgO8mwPmDcMf/oE6w7YK0R60HQeNwJrst5rsth/VN2avQiV7TbGj9gXP8sP0Uj0a1oH2jYn3mL84vf3QdDP8AmkfZKkT7JQI3T6GeOYnIC0vZeizF1hHZLZ3oNc1GMvMKeWnRXlrW8+KJm1tevnHtG8Yyen3+AV3usU2AjqB5P8xNe/GUaTELN++3dTR2Syd6TbORd1fs53RaDu/cFoqbS7G5ajb+H2ycbqyZ2u8lm8XnEERwGvg6dUmjyf7P9UjZMuhEr2k2sOVoMl9uPsH9PYMJb+b354bNH8Ga1yDkdhj6nu5hY43AcDJaDOMvTj/x0+87bR2NXdKJXtOqWHpuAc98t4ugup5MvtUyh7xSsP4dWPkitBtuLCJS3WekrETeQ17DTQrx3vJ/FBTp6YtL0ole06rYP5fEciY9l3/f2RlPVxdjaoOVL8H6N6HT3TB2TvVY3Lsq1W1BYstxjChaxW+/b7R1NHbHqkQvIoNE5ICIHBaR50vZLiIyw7J9t4iEFdv2hYicE5G9lRm4pjmiFXsTWbg9gcejWtClqR/kZcC342HLRxD5CIz8qHouB1gFGo96jWzxxHfDVJRZ1+qLu2aiFxFn4CNgMNAeGCciJSfaGAy0sjwmAR8X2zYHGFQZwWqaIzuXkcsLP+whpLEPf72lFSQfgc8HwsGVMPhdGPS2MRBIqxAnr7ocaPcEXQp2cHDj97YOx65Y87+qG3BYKXVUKZUPzAdGligzEvhSGbYAviLSEEAptQHQHVy1Gk0pxfML95CdX8S/7wjFtHsefHITpJ+GexZA5MP6xmsl6DjibxwhEN+Nr0Kh7oFzkTWJvjEQX+x1guW98pa5KhGZJCLRIhKdlJRUnl01ze7N+f04a/ef47V+frRc/xj8+Dg06gKPboIWN9s6vGrDw8OdvSEvUL/wNGdX/MvW4dgNaxJ9adWMkmONrSlzVUqpT5VSEUqpiICAgPLsqml2bcfJC0xfvot/NVzPHVvGwKFV0P9VmLAEfAJtHV6102/onfxCd/xiPjAmhNOsSvQJQPFFKQOBksu6WFNG02qc1PR01n05jfWmv3H7hU+R5n3h8a3Q+2+6++QNUtvdxLGIV8gzO5P5w1NG19UazppE/wfQSkSCRcQVuAtYUqLMEuA+S++b7kCaUiqxkmPVNMdx/jBq5RSc3+/A3ws/w6N+S5iwFMZ9A35Bto6u2rvz5q78h7vwStgAexfaOhybu2Y/LqVUoYg8AawEnIEvlFL7ROQRy/ZPgOXAEOAwkA3cf3F/EfkGiAL8RSQBmKqU+ryyL0TTbKowH07vgCNrYf9PcHYvZnFhQ2EYzt0eYtCw2/XN1irk6+lK7ZseYeeGX+mw7FlMwX3Bq+Y2CYs9Tu0ZERGhoqOjbR2Gpl1OKWO91vTTkH4KUuPhXCyc2Wsk+cIcQKBpD44G3MzdmxsR1qEtH90dhugkX+Wy8wuZ+M6XzC16Fpe2A5E751brD1sRiVFKRZS2TY/M0Kq3ghzIPAe5qZCfZXlklvg52yhXmGM8F1heX/bIhqwko3xxrl5Qrz2ET4BmvaBZLw5nuTJ65u80CvDgndtCdZK3EU9XF4YPuIV/LR3Li/u/gd3fQqe7bB2WTehErzkupYya9YXjcOGE5fm48V7mWSPB56Vf+zjiBCZPMHmAi4fxbPIw3nP3Ae8GxutaAVC7keXR2PIceNkgp+TMPO6fswk3F2c+nxiBt7ueysCW7urahCG/3c6w7J2ELP8H0qwn+Da1dVhVTid6zf4pBWkJkLQfzsX9+Xz+4OU1bHEyuiv6NIEGIeBV30jOXvXAw8+ofbt6Gcvxudb682cXt0r5Sp9bUMTDX8VwLj2Pbx/uQaCf53UfU7s+JmcnXh3ViSc+n8QqmYLb9xPh/hXg4mrr0KqUTvSa/TAXQepJI4EnHbA84ozn4gndqz4EtIHO443nOsFGTxafJjabDMxsVjy3cDfRJy7w0d1hdG7ia5M4tCv1aulPaEhnnombxIen/g2rXobB79g6rCqlE71WtZQymlRST0LqCUg+bCTy8weNnwtz/yxbqx7Uawud74aAtlCvndXutuIAACAASURBVPHsWcd28ZdCKcU/l+7jx52n+cetbRga2tDWIWklTBnanv4HkljueYwhWz+BJt2g4222DqvK6ESvlZ/ZbNy4zM+GgizjZuXFn/OzjRuXuWmQdd64gZlteU47ZTTBFBWfg0SMNtOANsa6qP6tjZ/9W9tdQi+NUop3Vhzgf5tPMKlPcx6LamHrkLRSNPBx55Vh7Xlq4RjC6h+nweLHwKcpNOlq69CqhE70NVVBDmQkQsaZPx85FyAvA3NeBmmpKWRnpKJy03EuzMLNnIObysXVnIvJnHvt41/k5gO16oKnv9Fu3naI8Qfm28RoaqnTHFwdsy1bKcXrP8XxxaZj3NO9KS8Mbqt72Nix2yMCWbnvDKMOP8aGOtNw/eYueGhNjRjApvvRV3cZZyBxt6Vp5BCctzyyzl1RVCHkOXmQWuROpnInEw+y8KTAxZNs3EkvciW9yJUc3MjBFd/aPrRq0oCOzRpQv24dI2GbahnPbt7GjVAXNxtc9I1XUGTmpUV7+C46gft7BfHy0PY4Oekkb+/OZeQy5IONdHA7y5yil5BaAXD/cuOGvYO7Wj96neirk/xsOBUNCdFwKsYYxJN+6s/tHnWMJhH/luAXDLUbYa5Vn1/POPPRtkxikhTe7q4MDW1En1b+hDfzI8Db7VItVSnFhewC9iemE33iAusOnGPHyVQA2jbwZlhoQ0Z1aVzte5uk5RTw+Nzt/Hb4PE/e0oq/9W+la/IOZPORZO75fCuPBJ1jctILiG9TY3oKB0/2OtFXV0UFRkI/tgGO/goJ26Ao39hWpzk0DodGYdCoM/i3MZpQitl7Ko2XFu9lV3wqLet58VhUC4aENMTdZP1kW6dTc1ix9wzL9yQSfeICAN2b12FMWCCDOzaodv3IY0+n88S87cRfyObN0SHcHtHk2jtpdmfWxqO8sSyOt8NSuevQZOM+0X1LwLu+rUOrMJ3oq5OsZDj0Cxz8GQ6vhfwMQIz27+Z9IagPBEZc9UZmWk4B7/1ygK+2nKBOLVdeGNyO0V0aX3fTQ3xKNot2nOKH7QkcT87G3eTErR0aMDy0Eb1b+ZfrA8TemM2KLzcf582f9+PnaeI/48LoFmz/N4u10imleHHRHr7ZFs/MnlkM2fMUeNaFu7+D+iUX0HMMOtE7MqWM7ocHf4YDK4xauzKDVwNofSu0vAWCbrKqh4pSih93nuaNZXGkZOVxb/dm/H1gG3w8KrfWrZRiR3wqP2xPYOmuRNJyCnA3OXFTqwAGtKtP71b+NPL1qNRz3kh7T6UxZfFedsan0q9NANNv70Rdr+p576EmKTIrHp+7nRX7zjCznxND9jxt9BgbOxta9bd1eOWmE72jKSqAE7/DwRVw4Ge4cMx4v0EotBkMrQdBw87lWl/08LkMpizey5ajKXQK9OGNUSGEBPrcoAv4U36hma3HklkVe5bVsWc5nWb02Gno405YMz/Cm/rRvlFtWgR44e/laldt3QfOZDBj7SGW7U7E38uVKUPbM7JzI7uKUbs+uQVFPD53O2v2n+PVPrWZcOI55Ow+6Pkk3PyyQ42g1YneEWQlw+FVRnI/vMaYo8XZDYL7QJtBRnKvwGpEmXmFfLj2MLM2HqWWmwvPDmrDXV2b4myDHiJKKWIT0/njWArRJy6w/cSFS4kfoLa7Cy3qedHI14N63m7Ur+1+2bOvpyu+niZMzjduAe3zmXmsjTvHgpgEth1PoZarMw/0DubBm5pX+jcfzT4UFJl5dsFuFu04xR2d6jLN8xtMO+ZAw04w/ANjyUcHoBO9PVIKzu4zEvvBlZDwB6CM4f2tBhqJvXkUuHlV6PAFRWbmbzvJ+6sPkZyVz9jwQJ4f3BZ/O2tyOJOWy8GzGRxJyjQe57I4m57L2fRcsvKLSt3Hy80FHw8Tvp4m/Dxd8fE04Vv8tYfp0odCbXcT7iYn3E3OuLk44eQk5OQXkZ1fRFZeISlZ+ZxIyWZ/Yjo741OJTUxHKWhW15PxkU25PbwJfrUcp1anVYzZrPhw3WH+vfogrep58Vm3szT7/SVjoF/YfXDzFLvvlaMTvb24cMLoIXN8o/GcYVmEq1EXI7G3vhUadCpXk0xJZrPil9izvLtiP0fPZ9EtuA4vDmnnkHOvZOYVci49l3MZeZzLyCM1O5/U7ALLI5/UnOLPxs/mCv539nZzIbSJD92C6nJLu3p0aFRbN9HUQBsPJfH373aRnJnHI5EBPOmyEPeYz4w5lMImQM8n7Hb2S53obaEgF87uNbo/ntoOJ3835ncBYyBR0E3Qop9Re/ducN2nS88tYOmu03zx2zGOJGXRsp4Xzw9qyy3t6tWYhGU2KzLzC0nLLuCC5UMhPbeA3AIzeYVF5BaYMZsVnm7OeLo64+nqgq+HiWZ1a1HP200PeNIAo1faOyv2M2/rSbzdXPhbuBPj8hbiEfe90RGixc3GhHqtbzVmP7UT153oRWQQ8AHGUoKzlFJvl9gulu1DMJYSnKiU2m7NvqVxqERvLjIm50o6aEyfe/6g0SRzdh+YC4wyXvUhsKvR3h7cx5iYqxKSb1p2Ab8eSmJV7Fl+2XeGvEIzHRvX5sHezRkW2hCXG9iWrWnVXVxiOh+sPsSKfWdwdhLGtlRMdPuVNmd+winj1J/30FrcDE0ijS7ONrx5e12JXkScgYPAACABY7HwcUqp2GJlhgB/xUj0kcAHSqlIa/Ytjc0Tvdls3AzNTTXmfyn+SE80RpumJRiP9FN/DlICo9tjQBtoHPbngKXaja4rsWfmFXImLZczabkcScpk/5kMdpy8wIGzGSgFdWq5MjSkIbeFB9Ip0KfG1OA1rSocO5/F/D9OsnjHKc6m5+GEmbF1jzHcfQ+h2ZvxyYkHQDm7InWaQ92Wfz78gqCWv9FH38Pvhk6jfb2JvgfwqlLqVsvrFwCUUm8VK/NfYL1S6hvL6wMYC4IHXWvf0lQ40S9+3DLNrTK+YinLM8ry88X3zUa5wrwrnwuyjSSvzGX8QpzAuxH4NLYschEIdVv9OeOiR8XbwqctiyUuMYPs/EKyLTcMkzPzrrgp6eNhIjTQh4hmdejdyp/OTXxt0otG02oSs1mx51QaGw4mEX3iAntOpZGSlU99UghzOkQnp6O0dE6khZwhkDOYKLziGFlOXhSZvKjt5QUu7sZcUMWfPevCyA8rFN/1rhnbGIgv9joBo9Z+rTKNrdz3YpCTgEmWl5mWDws7lQpc9UtJafyB85UVwW7g68o62PWr1GuzM/raHE+VXtcJYJvVpa1Y2pKPrrbxatfWrKydrEn0pVUVS34NKKuMNfsabyr1KfCpFfE4JBGJLuvT1tHpa3NM1fXaqut1QcWvzZpEnwAUn7kpEDhtZRlXK/bVNE3TbiBrumX8AbQSkWARcQXuApaUKLMEuE8M3YE0pVSilftqmqZpN9A1a/RKqUIReQJYidFF8gul1D4RecSy/RNgOUaPm8MY3Svvv9q+N+RK7F+1bZZCX5ujqq7XVl2vCyp4bXY5YErTHJmI3IQxZqSNrWPRNLCu6UbTbE5EjouIXc0dKyJBIqJEJLPYY5dSaqNO8po90YuDazWGZQS3KFXWIIkK81VKXdlpuvQYXKwtq2mVRdfoNYcmIn4i8pOIJInIBcvPgcW2rxeRaSKyCeP+UXMRGSgiB0QkTURmisivIvJgsX0eEJE4y/FWikiZ/ZPLiClKRBKKvT4uIs+JyG4gS0RaWr4J3C8i8ZbzPCIiXUVkt4ikisiHxfZvaYkxTUTOi8i31/VL02ocneg1R+cEzMYYLNIUyAFKDi28F2MwnjeQBiwAXgDqAgeAnhcLisgo4EVgDBAAbAS+qYQ4xwFDAV+4NGQyEmgF3Am8D7wE9Ac6AHeISF9LudeBXwA/jC7K/6mEeLQaRCd6zaEppZKVUguVUtlKqQxgGtC3RLE5Sql9liaTwcA+pdQPltczgDPFyj4MvKWUirNsfxPofI1a/XlLLTxVRCaXUWaGUipeKZVT7L3XlVK5SqlfgCzgG6XUOaXUKYwPmIsrXhRgfJA1spT/7Zq/GE0rRid6zaGJiKeI/FdETohIOrAB8LVMqHdR8Wk4GhV/rYxuZwnFtjcDPriYuIEUjBHeja8Shr9SytfymF5GmfhS3jtb7OecUl5fXHXmWUsM20Rkn4g8cJVYNO0K+mas5uieAdoAkUqpMyLSGdjB5dNvFO9DnIjR/AFcukFbfI3GeGCaUmpuJcdZ4X7MSqkzwEMAItIbWC0iG5RShysrOK160zV6zZGYRMS92MMFo909B0gVkTrA1GscYxkQIiKjLPs/DhRf+eUT4AUR6QAgIj4icnvlX4r1ROT2YjeYL2B8aJS+zqKmlUInes2RLMdI6hcfr2LcxPTAmNFvC7DiagdQSp0HbgfeBZKB9kA0kGfZvgh4B5hvaQrai9Gub0tdga0ikokxhchTSqljNo5JcyB6ZKxWo4mIE0Yb/Xil1Dpbx6NpN4Ku0Ws1jojcKiK+IuKG0ZVSML4NaFq1pBO9VhP1AI5gNPcMB0aV6PaoadWKbrrRNE2r5nSNXtM0rZqzy370/v7+KigoyNZhaJqmOYyYmJjzSqmA0rbZZaIPCgoiOjra1mFomqY5DBE5UdY23XSjaZpWzelEr2naNRWZFTn5ejCuo7LLphtN0+zDb4fOM+f3Y2w9lkJGbiEB3m50buLL3d2a0qd1AM5Ocu2DaDbnMIm+oKCAhIQEcnNzbR1KteDu7k5gYCAmk8nWoWh2KL/QzL9W7uezjcdo5OPOsNBGNPZ150RyNusOJLEq9izNA2rxj4FtGNSxAcbccJq9cphEn5CQgLe3N0FBQfo/1XVSSpGcnExCQgLBwcG2DkezM0op/vbdTpbtTuSe7k2ZMrQ97qY/Z33OLzSzYt8ZZqw5xKNztxMa6MOzt7aldyt/q45vNitOpGRzJi2X7PxCgv1r0axuLf3t4AZymESfm5urk3wlERHq1q1LUlKSrUPR7NB/1h5m2e5EnhvUlkejWlyx3dXFiRGdGjE0pCE/bE/g/dWHuOfzrZeadPq3r0+dWq6XyheZFcfOZ7L5aAqbj5xny9EUUrLyLzumr6eJJ/q15N4ezXBzcS55Su06OUyiB3SSr0T6d6mVZv2Bc7y36iBjujTmkb7Nr1rW2Um4PaIJIzo34ts/4vly8wmeXbgbFkJjXw98PEwUmRUnUrLILTDWY2/k405UmwAig+sQ6OeJu8mZI0mZ/LQ7kTeWxTF360m+fKAbTep4VsXl1hh2OQVCRESEKtmPPi4ujnbt2tkooupJ/0614nILihjw719xc3Hmp7/2vqy5xhpKKXbEp/LHsRT2nk4nJ78IEWhWx5M2DbzpFlyHpnU8y6xk/Howib/O2463u4n5k7rrZF9OIhKjlIoobZtD1egdwZAhQ5g3bx6+vr5llnnllVfo06cP/fv3L/fx169fz/Tp0/npp5+uJ0xNu8Invx4hPiWHeQ9FljvJg/EtMaypH2FN/Sp0/r6tA5j3UHfGz9rK+FlbWfZkb7zddWeByqATfSVRSqGUYvny5dcs+9prr1VBRJpmvfiUbD5ef4RhoQ3p2cK6m6oVlnIM9i+D4xsh/TSIE9RrD6G307F5Pz6fEMEd/93Mm8vjeGtM6I2NpYZwyET/z6X7iD2dXqnHbN+oNlOHd7hqmffee48vvvgCgAcffJBRo0YxePBg+vXrx+bNm1m8eDF9+/YlOjoaf39/Xn/9debOnUuTJk3w9/cnPDycyZMnM3HiRIYNG8bYsWMJCgpiwoQJLF26lIKCAr7//nvatm3Ltm3bePrpp8nJycHDw4PZs2fTpk2bSr1mTbvow7WHUcBLQ29gU15CNGx6H+J+AhT4twa/YDAXwMEVsGseBN1ExG2fM6lPCz759QgDOzSgX5t6Ny6mGsIhE70txMTEMHv2bLZu3YpSisjISPr27cuBAweYPXs2M2fOvKx8dHQ0CxcuZMeOHRQWFhIWFkZ4eHipx/b392f79u3MnDmT6dOnM2vWLNq2bcuGDRtwcXFh9erVvPjiiyxcuLAqLlWrYRIuZLNwewLjI5vS0Mejcg9uNsOhX2DTB3Dyd3D3gZv+DmETwK/Zn+UK8yDmf7B6KnwxkL+NW8ja/V689MMe1v0jSvfEuU4OmeivVfO+EX777TdGjx5NrVq1ABgzZgwbN26kWbNmdO/evdTyI0eOxMPD+MMZPnx4mcceM2YMAOHh4fzwww8ApKWlMWHCBA4dOoSIUFBQUNmXpGkAfLz+CE4iPFJKV8oKy02HXd/Ats8g+RD4NIFb34Kwe8HN+8ryLm4QOQkah8Hc23Gbfwf/vHUh476MZf62eCb0DKq82GogPdeNlcrqnXQx8VtbvjRubm4AODs7U1hYCMDLL79Mv3792Lt3L0uXLtUjgrUb4kxaLt9HJzA2IvD6a/OF+XB4DSx9Gt5rDz8/a9Tgx8yCJ3dAj8dKT/LFBUbAnV/DhWN03/cakUF+fLjusJ5n5zrpRG+lPn36sHjxYrKzs8nKymLRokXcdNNNZZbv3bv3pQSdmZnJsmXLynW+tLQ0GjduDMCcOXOuJ3RNK9NXW45TaDbzaN8K1ObNZji7D/74HBY8AP9qCV+Pgd3fQtuh8NBaeGgNhN4OzuXoPRPUC/q9hOz7gWltDpOUkcdXW46XPz7tEquabkRkEPAB4AzMUkq9XWK7WLYPAbKBiUqp7SLiDmwA3CznWqCUmlqJ8VeZsLAwJk6cSLdu3QDjZqyfX9ndyLp27cqIESPo1KkTzZo1IyIiAh8fH6vP9+yzzzJhwgTee+89br755uuOX9NKyi0o4ptt8fRvV9+6PuvmIkjcCcc2wInfIX4r5KYZ27zqQ7vhRoJv0Q9M1/ntoPffYN8iWu76Fze3nMmnG44xsWcwri66bloR1xwwJSLOwEFgAJAA/AGMU0rFFiszBPgrRqKPBD5QSkVaPgBqKaUyRcQE/AY8pZTacrVzVpcBU5mZmXh5eZGdnU2fPn349NNPCQsLs3VYlzji71SrPN9Hx/OPBbuZ92AkPVuW0aUyNw0O/AxxS+HYRsizJHb/1tC0h+XRHfyCoLJHWx9ZB1+N4nDnZ+m/pTMzxnVhRKdGlXuOauR6B0x1Aw4rpY5aDjYfGAnEFiszEvhSGZ8aW0TEV0QaKqUSgUxLGZPlYX9DcW+QSZMmERsbS25uLhMmTLCrJK/VbEop/rf5OK3re9GjRd3LN15M7vsWw5E1UJQPtRtDh1EQ3Md4eFVBl8cW/aDVrbTY/1/a1vmErzYf14m+gqxJ9I2B+GKvEzBq7dcq0xhItHwjiAFaAh8ppbZWPFzHMm/ePFuHoGml2n7yAntPpTNtdEdjSoKyknvXh4wE3zgCnGzQbNJnMvL5AF5pvYO7d3cm9nQ67RvVrvo4HJw1ib6072Mla+VlllFKFQGdRcQXWCQiHZVSe684icgkYBJA06ZNrQhL07SKmr3pOI3c8xnr8hvMe86+kntxTbpBYDciz32Hp6kzX205rkfLVoA1iT4BaFLsdSBwurxllFKpIrIeGARckeiVUp8Cn4LRRm9FXJqmlVdBLmnbFzJ6/yzed96Dy9IC+0vuJfV8Aufv7uPZZkf4v10uTB3eoUJz8dRk1iT6P4BWIhIMnALuAu4uUWYJ8ISl/T4SSFNKJYpIAFBgSfIeQH/gncoLX9M0q5w/BNGzYdc8fHIu0E7qkN3pfmqH326fyb24tsOgdiAji37h1bwWrIo9y3DdVl8u10z0SqlCEXkCWInRvfILpdQ+EXnEsv0TYDlGj5vDGN0r77fs3hD4n6Wd3gn4Timlp13UtKqScgzWvgF7F4CTC0VthvLEgU4UNu3NZ6NK3mqzU07O0HkcvhumE+J9L4t2nNKJvpys+hhXSi1XSrVWSrVQSk2zvPeJJcmjDI9btocopaIt7+9WSnVRSoUqpToqparVtI0PPvggsbGx1y5ohaCgIM6fP3/VMm+++Wa5jztnzhyeeOKJioalOSpzEWz+CGZ2hwPLofff4e9xLG75Jj9nt+W+XldfVMTudL4bQfFM/e38ejCJ85l5to7Iodjx9zX7N2vWLNq3b19l56tIotdqoNw0+GYcrHwRmkfBX2Og/1RUrQDm/H6cVvW86F1Wv3l7Vac5NOtFz/QVFJnNLN1V8jahdjUOOakZPz8PZ/ZU7jEbhMDgt8vcnJWVxR133EFCQgJFRUW8/PLLfPzxx0yfPp2IiAi8vLx4/PHHWb16NX5+frz55ps8++yznDx5kvfff58RI0YwZ84coqOj+fDDDwEYNmwYkydPJioq6rJzjRo1ivj4eHJzc3nqqaeYNGkSzz//PDk5OXTu3JkOHTowd+5cvv76a2bMmEF+fj6RkZHMnDkTZ2dnZs+ezVtvvUXDhg1p3br1pbl0tBog4wx8ORKSD8PQ/4OIv1wayBRz4gJ7TqX92aXS0XS+G9cfH2dkwBmW7vLj/l56YXtr6Rq9lVasWEGjRo3YtWsXe/fuZdCgQZdtz8rKIioqipiYGLy9vZkyZQqrVq1i0aJFvPLKK+U61xdffEFMTAzR0dHMmDGD5ORk3n77bTw8PNi5cydz584lLi6Ob7/9lk2bNrFz506cnZ2ZO3cuiYmJTJ06lU2bNrFq1apKa1rSHEB6IswZCqnxcO8i6PrgZaNVZ286Tm13F0Z3aWzDIK9D26HgZGKiz062n0zldGqOrSNyGI5Zo79KzftGCQkJYfLkyTz33HMMGzbsignNXF1dLyX/kJAQ3NzcMJlMhISEcPz48XKda8aMGSxatAiA+Ph4Dh06RN26l49eXLNmDTExMXTt2hWAnJwc6tWrx9atW4mKiiIgIACAO++8k4MHD1bkkjVHkpcBc8caNfp7fzCmJSjmdGoOK/ad4cHewXi6OuafPR5+0KIfHc+sAwaxYu8ZHuita/XWcNB/8arXunVrYmJiWL58OS+88AIDBw68bLvJZLr0ddjJyelSc4mTk9OlqYddXFwwm82X9ilt6uH169ezevVqNm/ejKenJ1FRUaWWU0oxYcIE3nrrrcveX7x4sWN+LdcqzlwE398P5+LgngVXJHmAr7acQCnFvT2alXIAB9JhNKZDvzDC/ww/762jE72VdNONlU6fPo2npyf33HMPkydPZvv27eU+RlBQEDt37sRsNhMfH8+2bduuKJOWloafnx+enp7s37+fLVv+nP/NZDJdWoDklltuYcGCBZw7dw6AlJQUTpw4QWRkJOvXryc5OfnS0oRaNffrO3B4FQydDi2unOk0J7+Ib7adZGD7BgT6WTFLpT1rM8TSfLOD6BMXOJuu12mwhk70VtqzZw/dunWjc+fOTJs2jSlTppT7GL169SI4OPhSM1Bpk5wNGjSIwsJCQkNDefnlly9bvWrSpEmEhoYyfvx42rdvzxtvvMHAgQMJDQ1lwIABJCYm0rBhQ1599VV69OhB//799URq1d3RX+HXd6HT3RDxQKlFftx5itTsAib2Cqra2G4ED19o3peOmZtQClbsPWPriBzCNacptoXqMk2xvdO/UweXmw4fRYKbFzy0znguQSnFoPc34uQkLH+yd/Vo1tv2GSyfzIRaH1Po15y5D17ZVFUTXW2aYl2j1zRHteY1yEiEUR+XmuQB1h04x4GzGTzQK6h6JHmA1rcCcF/d/Ww9mkJ6rl5P+Vp0otc0RxS/Df6YBZEPG+uslkIpxYdrD9PY14NRjtqlsjS+TaFee7rmb6PQrPj1QJKtI7J7DpXo7bGZyVHp36UDK8yHJU8as07eXPa9oi1HU9h+MpWH+zbH5OxQf+rX1vpWvM/9QVPPAlbHnbV1NHbPYf713d3dSU5O1gmqEiilSE5Oxt3d3dahaBWx6X1IioNh74Gbd5nFPlp3GH8vN+6IaFJmGYfVehBiLuTBRsdZt/8cBUXma+9TgzlMP/rAwEASEhJIStJf0yqDu7s7gYGBtg5DK68LJ2DDdOgw5lJbdWm2HUvht8PneXFI2+o5d3tgV/Dw4xbnHbyS24o/jqfQs4WDzd9ThRwm0ZtMJoKD9eAIrYZb8xqIEwx8o8wiSimmrzxAgLcb93YPqrrYqpKTM7QcQKPDa3B3uYM1ced0or8Kh2m60bQaLyHGmFe+5xPgU/bN1Q2HzrPteApP3twSD9dqWJu/qPWtSE4y4wOTWB13VjfrXoVO9JrmCJSCX16CWvWg11NlFjObFf9auZ9APw/u7FrN115ueQuIM6M893IiOZvD5zJtHZHd0ole0xxB3FI4uRn6vXjVG7ALtiew91Q6kwe2wdWlmv95e/hB0+60Td8EwCrd+6ZMVv1PEJFBInJARA6LyPOlbBcRmWHZvltEwizvNxGRdSISJyL7RKTsqoimaaUrzIfVUyGgHXS5t8xiGbkFvLviAGFNfRnZuYYstddqIKbzsfRtWMjqWJ3oy3LNRG9Z7/UjYDDQHhgnIiWXVRoMtLI8JgEfW94vBJ5RSrUDugOPl7KvpmlXE/05pByFga+Dc9n9Jz5ce5jzmXlMHd6h+oyCvZaW/QEY73+IHfGpeonBMlhTo+8GHFZKHVVK5QPzgZElyowEvrSsHbsF8BWRhkqpRKXUdgClVAYQB1SjIXqadoPlXDBmp2ze71JSK82x81l8sekYt4cH0qmJbxUGaGP1O4BXAyKLdqAUrI07Z+uI7JI1ib4xEF/sdQJXJutrlhGRIKALsLW8QWpajbVhOuSkGrX5q9TSpy2LxdXZiX8MavP/7d15fJTltcDx35lksq+sAcISIOxSWUXQq7JYoLggRdGqqG0Vt9re3ttqW9veLletVqu2Yq3a1laxtMoVl5atgCK7CmhYI/saAmRfJ3PuH++LHWkSJiHJZCbn+/nMJ5n3fd7JeUhyePPM85ynBYNrBUSg7wRSDq+iW4qXZdtt+KY2wST62n66zpzHVG8bEUkCXgO+qapFtX4RfGeWlAAAFrlJREFUkdtFZKOIbLRFUcYAJ/fA+udg2FecPY3rsHLncZZuy+PeCdl0Sm6Dq537TkAqCripxwne25VPRXVNqCNqdYJJ9AeBwDXUmcCZW7DX2UZEvDhJ/mVVfb2uL6Kqz6nqSFUdeXobPGPatGX/A55ouKzuejbVNX5++tZWerZP4NZIqDffGL0vA/HwxdgcyqpqWLv7RKgjanWCSfQbgGwRyRKRGGAWsPCMNguBm93ZN2OAQlU9Is47Qi8A21T18SaN3JhIdmA95CyAsd+AlC51NvvTmn3k5pXwgy8NIjY6ghdH1SehHXQbQc9Tq4n3RrHMxun/zVkTvar6gHuARThvps5X1RwRmSMic9xm7wC7gVzgd8Bd7vFxwE3AeBHZ5D6mNnUnjIkofj/84wFIyoCx99bZ7ERJJU8s3cnF2R2YOLBTCwbYCvWdiOfwh1ye5eWf2/NslewZgqp1o6rv4CTzwGPPBnyuwN21XLeK2sfvjTF1+fivcGhjvRuKADy+ZCdlVTX8cNqgtjOdsi59J8KKh7iuXS5v7Mxk+9FiBnZJCXVUrUaEL50zJsxUlcLSH0OX82HorDqb7ThazLz1+7nxgh5kd657pWyb0XUYxKczrOoDAJbZKtnPsURvTGvy/lNQfBgmPwye2n89VZWfvb2VpNhovjmxXwsH2Ep5oqDPeOL3Lef8bskstXH6z7FEb0xrUXgQ3n8SBk+HnhfW2WzFzuO8tyufb0zIJj0xpgUDbOX6ToTSPK7tUcTmgwUcL7ZVsqdZojemtVj8A1A/TPpJnU2qa/z8/O1tZHVI5OYLe7VcbOGgz3gAJni3oOpsjG4cluiNaQ12LnamU178bWfz6zrMW7+f3LwSHpgyIPKrUzZUcgZknEenY6vokhpn4/QB7CfFmFCrKoW3vw0d+sFF36yzWWF5NU8s2cmY3u2YNKhzCwYYRvpORA6sY0p2oq2SDWCJ3phQW/EQFO6HK56E6Ng6mz2zIpeC8moetOmUdes7Efw+pqd9SllVDWtslSxgid6Y0DqyBdY8A8Nvhp5j62x2tLCCP7y/l+nnd2Nw19QWDDDMZI6GmGQGlq4jMSaKxTk2fAOW6I0JHV8VLLzHWcJfzxuwAE8u24VflW9NsumU9YqOgT6XEp27hEv7dWTJ1mPU+G2VrCV6Y0Jl+c/hyGaY9oSzLV4d9uSXMn/jAW4Y3YPu7RJaMMAw1X8qFB/m2m755JdU8tH+U6GOKOQs0RsTCnvedebMD58NA6+ot+kvF+8gNtrDPeOzWyi4MNdvMoiHMdXr8EYJi3KOhjqikLNEb0xLKzsJr98B7fvA5IfqbfrJoULe2nKE28Zl0TG57jdqTYCEdtBjLLG5/2Bsnw4syjnW5oucWaI3piWpwpv3QelxmPE8xCTW2/zRRTtIS/By+yW9WyjACDFgKuTlcE1WNftPlrHtSHGoIwopS/TGtKTVT8G2hTDhQacQVz3W7j7Byp3HufOSPqTEeVsowAjR36mGPkE24hF45+MjIQ4otCzRG9NSdi2BJT+CQVc7G4rUQ1V5+O/byUiJY/bYXi0TXyRplwWdBpG0ZzFj+3TgrS2H2/TwjSV6Y1pC/i7421chYwhc/Uy9G30DLMo5yqYDBXxrUjZx3ja6c9S56j8V9q9m+oB49p4oI+dwrdtVtwmW6I1pbuUFMO96iIqGWa+cdVzeV+PnF4t20KdjIjOGZ7ZQkBFowFRQP5O9m4nyCG+34eEbS/TGNKfqcifJn9oL175Ub8Gy0/76wUF2Hy/lO5MHEB1lv6KN1mUYJHclce8ixvVt28M3Qf0UichkEdkhIrkicn8t50VEnnLPbxGR4QHnXhSRPBH5pCkDN6bVq/HBa1+D/Wvgmt9Cr4vOekl5VQ2/WrqT4T3SuNwKl50bj8e5q89dxlWDUjlwspxNBwpCHVVInDXRi0gU8BtgCjAIuF5EBp3RbAqQ7T5uB+YGnPsDMLkpgjUmbKjC2/8J29+CKY/AkBlBXfb71Xs4VlTJ/VMGWuGypjD4GqguY0rMR8RGe3j9w0OhjigkgrmjHw3kqupuVa0CXgWuOqPNVcBL6lgLpIlIFwBVfRc42ZRBG9OqqcLSH8GHf3Tqy19wR1CXnSqtYu6KT5kwoBOjs9o1c5BtRI8LIaUbCdsXcPngDN7ccphKX9srXRxMou8GHAh4ftA91tA29RKR20Vko4hsPH78eEMuNab1UIV//swpbzDyNhj/YNCX/nLJDsqqavjO5AHNGGAb4/HAkGsgdynXDU6goKya5dvb3s5TwST62v5+PPMdjWDa1EtVn1PVkao6smPHjg251JjWY+Uj8N5jTtnhqb886zTK0z45VMgr6/Zz05ie9M9IbuYg25jzZoLfx4UVq+iYHMtrbXD4JphEfxDoHvA8EzjciDbGRLaVjzqbiJz/FZj2pHM3GQRV5ccLc0hPiLEyxM0hYyh0HEDUlnlMH9aN5dvz2tzG4cH8JG4AskUkS0RigFnAwjPaLARudmffjAEKVbXtTlo1bc+qJ2D5z2DodXDl00EneYBX1u9n475TfHfKAFLjrdRBkxNx/sI6uIEbs0rx+ZX5Gw+c/boIctafRlX1AfcAi4BtwHxVzRGROSIyx232DrAbyAV+B9x1+noRmQesAfqLyEER+WoT98GY0Fr9NCz9sTOz5uq54Al+JeuBk2X8/O1tXNS3AzNH2OKoZjN0FkTF0GPfa4zt055X1u1vUxuSRAfTSFXfwUnmgceeDfhcgbvruPb6cwnQmFZt7VxY/AOnfs305xqU5P1+5Tt/24JHhEe+PNSmUzanxPYwYBpsnsfsL97BHa/msHx7HhPbyFoFW3ZnTGO9/xT8434ngcx43ilx0AC/WraLNbtP8OC0gXRLi2+mIM1nRtwC5aeYUPMenVNi+dPafaGOqMVYojemMVY+CksehMHTYeYfIKphY+tLtx7jqWW7mDE8k2tHdj/7BebcZf0HdB5C9Lq53DCqByt3HmfXsbZRp94SvTENoQrLfuq+8ToLrnm+wUl+84ECvvWXTQzplsLPpw+xIZuWIgJj7oK8HG7tuo94bxRzV34a6qhahCV6Y4Kl6ozHn54nf/UzDR6u+eRQITe9sI60RC+/u3mklSBuaed9GRI7kfLBb7jhgh68sekwB06WhTqqZmeJ3phg+Gvg7W/Dml/DqK+78+QblqSXbj3G9b9bS3Kcl3lfH0OXVBuXb3HRsTD2Xti9nDt75+ER+O27kX9Xb4nemLOproC/zoaNL8C4+2Dqow2aJ19a6eOhd7bxtZc20rN9An+5YwyZ6QnNGLCp16ivQVJnOqx/jBnDM5m/4WDE39VbojemPuWn4E/TYdub8MWHYNJPgi5rUFRRzUtr9nLZYyv47bu7uX50d/42Z6wl+VCLSXCKze19j//OPoTHA4/8Y3uoo2pWDRtgNKYtyd8Fr34FTu2BL79Yb6nhGr+SV1zBvhNlfHKokPV7TrJy53EqfX6G90hj7o0jGNEzvQWDN/UacQuse5b2Kx/kzov+wBPL93HruFMR+z2S1rjjysiRI3Xjxo2hDsNEqEpfDXvzy8jNK+FYUQUFZVWcLKuioKyaimo/Pr+foSWrmHPyUaqJ5uHk+9kUNZQavx+fX/H7FZ9fqXEfFdU1FFX4Pvc1ureL55J+HZk5ojtDM1NtZk1rtGspvDyDqksf5KL3z6dLahyv3zWOKE94fq9E5ANVHVnbObujNxHvwMkyVu48ztrdJ9h6uIh9J8s+t/xdBNLivaQlxJAYrdxcOY9ry//Cp95snu7wI4piM8j0CNEeISrg4Tz3EBvtIS3BS4ekWHq2TyC7UzIZqXEh7LEJSvZEGDCNmFWP8vB/zOe2dwr57bufctelfUMdWZOzO3oTkY4WVvDWlsO8ueUIm93t4zJS4ji/exrZnZPo28l5dE2NJyXe69zFHf4I3rgXjn0Mw250ygx7LWFHtOJjMHcsmtSJ+5If5+/bT7HwnosY2CUl1JE1WH139JboTcRQVd7blc8Lq/bw7q7jqMKQbilcMbQrlw/OoFf7hNqHUCpL4N1fwOpfQ2IH+NIvYeAVLd8BExo7F8MrM6kccj3jts0gPTGG1+4aS0pceFUStaEbE9EqqmtYuOkwz6/azc5jJXRMjuUb47O56vyu9O6YVM+FRfDhS06J4bJ8ZxHUpJ9CfFrLBW9Cr9/lcMl3iV35CK8P7cz4jRdw98sf8uIto/BGRcbEREv0Jmzll1Ty57X7+PPafeSXVDEgI5nHZn6BK77QhdjoOhYz1fhg3yrIWQCfvA6VRU4NlAk/gsxab4ZMW3DpA1BwgB6bf8WC877BFZsv4LuvbeEXM4YSHQHJ3hK9CSuqypaDzrZ7CzYdosrnZ/yATnztoiwu7NP+34dmaqohbysc3AAHNkDuUufu3ZsIA6c5G3d3GxGazpjWQwSufAq0hvO2PMXCrJl8+cNpFJX7+PUNw8K+VIUlekNZlY+ich/VNX73oURHCUmx0STGRpPgjcIT4ilneUUVLPjoEH/74CC78kqI83qYOSKTW8dl0TfZB6f2wtZVzsfAR+EB8LtTHxM7OXfvg6dD9iTwWgkCEyDKC1c/C0mdGLr6adZ0+oQbd8zmmmfKefy6LzAgI/zeoD3N3oxtIwrLq9l1rJgdx4rZcbSYvSfKOFpYztHCin+bA34mEUiJ85KW4ExBTE/wfjYdMS3BS7r78fS51Hgv8TFRxHudR0P/9PX7lWPFFeQcKmLTgQLe23WcvYcO0YujXNqxhImdS+gfk0dMwR44udtZvRoooT2k93IeaT0hYwhkjoLU7kGvajVt3Pa34Y170IoCFjCeZ6qmMunicXz94t60S4wJdXS1OudZNyIyGXgSiAKeV9WHzzgv7vmpQBlwi6p+GMy1tbFE33hFFdXsPl7Kp3kl7AxI7EcKKz5rkxQbTe+OiXRJjSMjJY6M1HhS4714o4SYaA/RHg/VNX5KKn2UVvooqfRRWF5NQVk1p9yFRQXlVRSUVlNcWf9/EgDeKCHOTfqn/wMIfF7jV8qrapCKAtIr9pNYup9u/qP08hyllxylb1QeKVoU8IoCqZnQrje07+N8DEzsceF752VakbKTsOJh9IPfozXVrPYPZjEXktr/EsaMHsOorPbERLee8ftzSvQiEgXsBCYBB3E2C79eVbcGtJkK3IuT6C8AnlTVC4K5tjZtMdH7/UqNOistVcGv6jz8UOGr+SzpllbWfJZ880sqOVpYwbHiSo4WlrMnv5T8kqrPXjMmykOfTkkMyEimX+dk+mck0T8jha6pcU2zUlOVap+PwrIKCkudR1FpBcXllVRUVeGrqqCmvBitLEarSpCqEjxVJXiqS0ioOkFKdT7JvhO095+kgz+fZP3XJhCKUJXYhegOvYnq0Bfa9XGTeh8nodv8dtNSio/Bxhep2vQqMYV7ASjQRLbQl5LEXpCaiSe9O8npnUlPTyc+MY3YhGRik1JJiIsnLtaLeLwNKoTXGOc6vXI0kKuqu90XexW4CghM1lcBL7l7x64VkTQR6QL0CuJaA3xr/ibe2HS4wdfFeT1kpMTRKSWOCQM6k9Uxkd4dEundMYle7ROab8bAgjmweR5eoIP7aBCJgqTOkN4ZkgdCcoZzZ97OuUOX9F7EWjI3rUFyZ7jsAWIuvR+O76B631pKtq6i39GPSC1fTHxZORwJ4nW+cANMn9vs4dYmmETfDTgQ8Pwgzl372dp0C/JaAETkduB292mJiOwIIrZw0gHIb44XbgX/UI3s2ymg1VcNbLbvWysQqX1rpf161n2ck/r61rOui4JJ9LX9jX/meE9dbYK51jmo+hzwXBDxhCUR2VjXn1XhzvoWniK1b5HaL2h834JJ9AeBwN2LM4EzxxjqahMTxLXGGGOaUTADuBuAbBHJEpEYYBaw8Iw2C4GbxTEGKFTVI0Fea4wxphmd9Y5eVX0icg+wCGeK5IuqmiMic9zzzwLv4My4ycWZXnlrfdc2S09av4gdlsL6Fq4itW+R2i9oZN9a5YIpY4wxTaf1zPY3xhjTLCzRG2NMhLNE3wJE5F4R2SEiOSLyi4DjD4hIrnvui6GM8VyIyH+JiIpIh4BjYds3EXlURLaLyBYRWSAiaQHnwrZfp4nIZDf+XBG5P9TxnAsR6S4iy0Vkm/v7dZ97vJ2ILBGRXe7HsNz1W0SiROQjEXnLfd6oflmib2YichnOauChqjoYeMw9PghnFtJgYDLwjFsyIqyISHecEhf7A46Fe9+WAENUdShOCY8HICL6dbqkyW+AKcAg4Hq3X+HKB3xbVQcCY4C73f7cDyxT1Wxgmfs8HN0HbAt43qh+WaJvfncCD6tqJYCq5rnHrwJeVdVKVd2DM2NpdIhiPBdPAN/h8wvhwrpvqrpYVU9Xa1uLs/4Dwrxfrs9KmqhqFXC6LElYUtUjpwsoqmoxTlLshtOnP7rN/ghcHZoIG09EMoEvAc8HHG5UvyzRN79+wMUisk5EVorIKPd4XWUjwoaIXAkcUtXNZ5wK+74FuA34u/t5JPQrEvpQKxHpBQwD1gGd3bU8uB87hS6yRvsVzk2UP+BYo/plG480ARFZCmTUcur7OP/G6Th/Vo4C5otIbxpQHiKUztK37wGX13ZZLcdaVd/q65eqvuG2+T7O0MDLpy+rpX2r6lcQIqEP/0ZEkoDXgG+qalGTVGcNIRGZBuSp6gcicum5vp4l+iagqhPrOicidwKvu5U914uIH6cwUTClJUKurr6JyHlAFrDZ/aXKBD4UkdGEQd/q+54BiMhsYBowQf+12KTV9ysIkdCHzxERL06Sf1lVX3cPHxORLqp6xK2km1f3K7RK44Ar3RLwcUCKiPyZRvbLhm6a3/8B4wFEpB9O/Z98nFIQs0QkVkSygGxgfciibCBV/VhVO6lqL1XthZNAhqvqUcK8b+5mOd8FrlTVsoBTYd0vV0SVJRHnLuMFYJuqPh5waiEw2/18NvBGS8d2LlT1AVXNdH+3ZgH/VNUbaWS/7I6++b0IvCginwBVwGz3DjFHRObj1Ob3AXerak0I42wybomMcO7br4FYYIn718paVZ0TAf2KxLIk44CbgI9FZJN77HvAwzjDpF/FmRE2M0TxNbVG9ctKIBhjTISzoRtjjIlwluiNMSbCWaI3xpgIZ4neGGMinCV6Y4yJcJboTZsmIt93qx5uEZFNInJBqGMypqnZPHrTZonIhTirX4eraqVbZjnmHF4vOqAYmjGtht3Rm7asC5AfUFk0X1UPi8goEVktIptFZL2IJItInIj8XkQ+duuDXwYgIreIyF9F5E1gsYgkisiLIrLBbRe2lSFN5LA7etOWLQZ+KCI7gaXAX4A17sfrVHWDiKQA5Th1wVHV80RkAE5S7+e+zoU4+w2cFJH/xVmufpu7Ycl6EVmqqqUt3DdjPmN39KbNUtUSYARwO3AcJ8HfARxR1Q1umyJ3OOYi4E/use3APpwS1ABLVPWk+/nlwP3ucvwVOAWperRIh4ypg93RmzbNrVWzAlghIh8Dd1N72d766t4G3q0LMENVdzRZkMacI7ujN22WiPQXkeyAQ+fj7FDU9fQGMe74fDTwLvAV91g/nLv02pL5IuBet6oiIjKsGbtgTFDsjt60ZUnA0+5Yug9na8Dbgd+7x+NxxucnAs8Az7p3/T7gFnemzpmv+VOcnYG2uMl+L87MHmNCxqpXGmNMhLOhG2OMiXCW6I0xJsJZojfGmAhnid4YYyKcJXpjjIlwluiNMSbCWaI3xpgI9/8f9pDccGi6pAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scores of original and simulated data\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "for size, number in [(\"Small Firms\", 0), (\"Large Firms\", 1)]:\n",
    "    ax[number].set_title(size)\n",
    "    ax[1].set_xlabel(\"Score\")\n",
    "    sns.kdeplot(original_data.loc[original_data[\"largem\"]==number, \"s\"], ax=ax[number], label=\"original\")\n",
    "    sns.kdeplot(simulated_data.loc[simulated_data[\"large\"]==number, \"score\"], ax=ax[number], label=\"simulated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original paper, the authors run the polynomial regression with $P \\in \\{0, 1, 2, 3\\}$ and choose the optimal model according to the AIC which is readily available when running a linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bronzini, R., & Iachini, E. (2014). Are incentives for R&D effective? Evidence from a regression discontinuity approach. *American Economic Journal: Economic Policy, 6*(4), 100-134.\n",
    "\n",
    "Button, P. (2016). Model uncertainty and model averaging in regression discontinuity designs. *Journal of Econometric Methods, 5*(1), 103-116.\n",
    "\n",
    "Fletcher, D. (2019). Model Averaging. Springer.\n",
    "\n",
    "Hansen, B. E., & Racine, J. S. (2012). Jackknife model averaging. *Journal of Econometrics, 167*(1), 38-46.\n",
    "\n",
    "Imbens, G. W., & Lemieux, T. (2008). Regression discontinuity designs: A guide to practice. *Journal of econometrics, 142*(2), 615-635.\n",
    "\n",
    "Lee, D. S., & Lemieux, T. (2010). Regression discontinuity designs in economics. *Journal of economic literature, 48*(2), 281-355.\n",
    "\n",
    "Li, K. C. (1987). Asymptotic optimality for Cp, CL, cross-validation and generalized cross-validation: discrete index set. *The Annals of Statistics, 958-975*.\n",
    "\n",
    "Moral‐Benito, E. (2015). Model averaging in economics: An overview. *Journal of Economic Surveys, 29*(1), 46-75.\n",
    "\n",
    "Steel, M. F. (forthcoming). Model Averaging and Its Use in Economics. *Journal of Economic Literature*.\n",
    "\n",
    "Stone, M. (1974). Cross‐validatory choice and assessment of statistical predictions. *Journal of the Royal Statistical Society: Series B (Methodological), 36*(2), 111-133.\n",
    "\n",
    "Wolpert, D. H. (1992). Stacked generalization. *Neural networks, 5*(2), 241-259."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
